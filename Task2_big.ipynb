{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import dump, load\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "X = read_csv('features-bulk.csv')\n",
    "y = read_csv('target-bulk.csv')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_standardized = scaler.fit_transform(X_train)\n",
    "X_test_standardized = scaler.transform(X_test)\n",
    "\n",
    "def calculate_r2(y_true, pred_y):\n",
    "    \n",
    "    #Calculate the R-squared score.\n",
    "    mean_y_true = sum(y_true) / len(y_true)\n",
    "    total_sum_of_squares = 0\n",
    "    for y_i in y_true:\n",
    "       total_sum_of_squares += (y_i - mean_y_true) ** 2\n",
    "    # Check if total_sum_of_squares is zero\n",
    "    if total_sum_of_squares == 0:\n",
    "        r2 = 0  # or set to another appropriate value\n",
    "    else:\n",
    "        residual_sum_of_squares = 0\n",
    "        for y_j, y_pred_j in zip(y_true, pred_y):\n",
    "            residual_sum_of_squares +=((y_j - y_pred_j) ** 2 )\n",
    "            r2 = 1 - (residual_sum_of_squares / total_sum_of_squares)\n",
    "    return r2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculating the R-squared (linear_r2) using predictions on both the training and test sets helps in assessing the model's overall performance, including its ability to generalize to new, unseen data. In practice, it's common to evaluate models on both training and test sets to understand their behavior and identify potential issues like overfitting or underfitting.\n",
    " \n",
    "The coefficient of determination (R-squared) is a measure that indicates the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, where:\n",
    "\n",
    "R-squared close to 1 indicates that a large proportion of the variance in the dependent variable is explained by the independent variables, suggesting a good fit.\n",
    "R-squared close to 0 indicates that the model does not explain much of the variance in the dependent variable, suggesting a poor fit.\n",
    "However, there isn't a universally agreed-upon threshold for what constitutes a \"good\" R-squared value, as it depends on the context and the nature of the data. In general terms:\n",
    "\n",
    "0.7 to 1.0: A high R-squared value. The model is a good fit, and a large proportion of the variance is explained.\n",
    "0.5 to 0.7: A moderate R-squared value. The model explains a moderate amount of the variance.\n",
    "Below 0.5: A low R-squared value. The model might not be explaining much of the variance, and its predictive power might be limited.\n",
    "However, it's crucial to interpret R-squared in the context of the specific problem and the nature of the data. R-squared alone doesn't provide information about the correctness of the model specification, the significance of individual predictors, or the presence of outliers.\n",
    "\n",
    "Always consider other metrics and aspects of model performance in conjunction with R-squared, and be cautious about overfitting or relying solely on one metric for model evaluation. Additionally, when comparing models, it's often helpful to use domain-specific knowledge and assess the model's performance against a baseline or other relevant benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Least Square Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized Linear Regression R-squared: -0.806748500561179\n",
      "Non-standardized Linear Regression R-squared: -0.7975119361750378\n"
     ]
    }
   ],
   "source": [
    "standardized_model = LinearRegression()\n",
    "standardized_model.fit(X_train_standardized, y_train)\n",
    "standardized_pred_y = standardized_model.predict(X_train_standardized)\n",
    "'''evaluate the model's performance on the training data itself, \n",
    "providing insights into how well the model fits the training set.'''\n",
    "standardized_y_true = y_test['K_VRH']\n",
    "Standardized_linear_r2 = calculate_r2(standardized_y_true, standardized_pred_y)\n",
    "print(\"Standardized Linear Regression R-squared:\", Standardized_linear_r2[0])\n",
    "\n",
    "non_standardized_model = LinearRegression()\n",
    "non_standardized_model.fit(X_train, y_train)\n",
    "non_standardized_pred_y = non_standardized_model.predict(X_train)\n",
    "'''evaluate the model's performance on the training data itself, \n",
    "providing insights into how well the model fits the training set.'''\n",
    "non_standardized_y_true = y_test['K_VRH']\n",
    "Non_standardized_linear_r2 = calculate_r2(non_standardized_y_true, non_standardized_pred_y)\n",
    "print(\"Non-standardized Linear Regression R-squared:\", Non_standardized_linear_r2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression:\n",
    "## Objective Function: \n",
    "Minimizes the sum of squared differences with the addition of the absolute values of the coefficients multiplied by a regularization parameter (L1 regularization).\n",
    "## Regularization: \n",
    "Encourages sparsity in the coefficient values, leading to some coefficients being exactly zero.\n",
    "## Outcome: \n",
    "Can be effective in feature selection, setting some coefficients to zero and thus excluding irrelevant features.\n",
    "## Resulting Coefficients: \n",
    "Tends to yield sparse coefficient vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized Lasso Regression R-squared: -0.7758675145187031\n",
      "lasso_important_features for standardized data Index(['MagpieData mean Number', 'MagpieData mean MeltingT',\n",
      "       'MagpieData mean CovalentRadius', 'MagpieData mean NfValence',\n",
      "       'MagpieData mean NValence', 'MagpieData minimum GSvolume_pa',\n",
      "       'MagpieData mean GSvolume_pa', 'MagpieData mean GSmagmom', 'density',\n",
      "       'packing fraction'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "lasso_params = {'alpha': [0.01, 0.1, 1, 10]}\n",
    "standardized_lasso_grid = GridSearchCV(Lasso(max_iter=100000), lasso_params, cv=5)\n",
    "standardized_lasso_grid.fit(X_train_standardized, y_train)\n",
    "y_true = y_test['K_VRH']\n",
    "standardized_lasso_y_pred = standardized_lasso_grid.predict(X_train_standardized)\n",
    "Standardized_lasso_r2 = calculate_r2(y_true, standardized_lasso_y_pred)\n",
    "print(\"Standardized Lasso Regression R-squared:\", Standardized_lasso_r2)\n",
    "standardized_lasso_selector = SelectFromModel(standardized_lasso_grid.best_estimator_, max_features=10)\n",
    "standardized_lasso_selector.fit(X_train_standardized, y_train)\n",
    "standardized_lasso_important_features = X.columns[standardized_lasso_selector.get_support()]\n",
    "print(\"lasso_important_features for standardized data\",standardized_lasso_important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.861e+04, tolerance: 4.255e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.814e+04, tolerance: 3.995e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.758e+04, tolerance: 3.915e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.794e+04, tolerance: 3.857e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.190e+04, tolerance: 4.000e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.574e+04, tolerance: 5.008e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-standardized Lasso Regression R-squared: -0.7892461765408068\n",
      "lasso_important_features for Non-standardized data Index(['MagpieData mean Number', 'MagpieData mean Row',\n",
      "       'MagpieData mean Electronegativity', 'MagpieData minimum NpValence',\n",
      "       'MagpieData mean NpValence', 'MagpieData minimum GSmagmom',\n",
      "       'MagpieData mean GSmagmom', 'MagpieData avg_dev GSmagmom', 'density',\n",
      "       'packing fraction'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.574e+04, tolerance: 5.008e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "non_standardized_lasso_grid = GridSearchCV(Lasso(max_iter=100000), lasso_params, cv=5)\n",
    "non_standardized_lasso_grid.fit(X_train, y_train)\n",
    "non_standardized_lasso_y_pred = non_standardized_lasso_grid.predict(X_train)\n",
    "Non_standardized_lasso_r2 = calculate_r2(y_true, non_standardized_lasso_y_pred)\n",
    "print(\"Non-standardized Lasso Regression R-squared:\", Non_standardized_lasso_r2)\n",
    "non_standardized_lasso_selector = SelectFromModel(non_standardized_lasso_grid.best_estimator_, max_features=10)\n",
    "non_standardized_lasso_selector.fit(X_train, y_train)\n",
    "non_standardized_lasso_important_features = X.columns[non_standardized_lasso_selector.get_support()]\n",
    "print(\"lasso_important_features for Non-standardized data\",non_standardized_lasso_important_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression:\n",
    "\n",
    "## Objective Function: \n",
    "Minimizes the sum of squared differences with the addition of the squared values of the coefficients multiplied by a regularization parameter (L2 regularization).\n",
    "## Regularization: \n",
    "Encourages smaller but non-zero coefficients for all features, helping to mitigate multicollinearity issues.\n",
    "## Outcome: \n",
    "May not result in exact feature selection, but it can be more stable when there are highly correlated features.\n",
    "## Resulting Coefficients: \n",
    "Tends to produce non-sparse coefficient vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized Ridge Regression R-squared: -0.7907004182189743\n",
      "ridge_important_features for standardized data Index(['MagpieData maximum Number', 'MagpieData mean Number',\n",
      "       'MagpieData avg_dev Number', 'MagpieData maximum AtomicWeight',\n",
      "       'MagpieData mean AtomicWeight', 'MagpieData avg_dev AtomicWeight',\n",
      "       'MagpieData mean Row', 'MagpieData mean CovalentRadius',\n",
      "       'MagpieData mean NpValence', 'density'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "ridge_params = {'alpha': [0.01, 0.1, 1, 10]}\n",
    "standardized_ridge_grid = GridSearchCV(Ridge(max_iter=100000), ridge_params, cv=5)\n",
    "standardized_ridge_grid.fit(X_train_standardized, y_train)\n",
    "standardized_ridge_y_pred = standardized_ridge_grid.predict(X_train_standardized)\n",
    "Standardized_ridge_r2 = calculate_r2(y_true, standardized_ridge_y_pred)\n",
    "print(\"Standardized Ridge Regression R-squared:\", Standardized_ridge_r2[0])\n",
    "standardized_ridge_selector = SelectFromModel(standardized_ridge_grid.best_estimator_, max_features=10)\n",
    "standardized_ridge_selector.fit(X_train_standardized, y_train)\n",
    "standardized_ridge_important_features = X.columns[standardized_ridge_selector.get_support()]\n",
    "print(\"ridge_important_features for standardized data\",standardized_ridge_important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-standardized Ridge Regression R-squared: -0.7898305765276765\n",
      "ridge_important_features for Non-standardized data Index(['MagpieData mean Number', 'MagpieData minimum Row',\n",
      "       'MagpieData maximum Row', 'MagpieData mean Row',\n",
      "       'MagpieData minimum NpValence', 'MagpieData mean NpValence',\n",
      "       'MagpieData mean GSmagmom', 'MagpieData avg_dev GSmagmom', 'density',\n",
      "       'packing fraction'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "non_standardized_ridge_grid = GridSearchCV(Ridge(max_iter=100000), ridge_params, cv=5)\n",
    "non_standardized_ridge_grid.fit(X_train, y_train)\n",
    "non_standardized_ridge_y_pred = non_standardized_ridge_grid.predict(X_train)\n",
    "Non_standardized_rifge_ridge_r2 = calculate_r2(y_true, non_standardized_ridge_y_pred)\n",
    "print(\"Non-standardized Ridge Regression R-squared:\", Non_standardized_rifge_ridge_r2[0])\n",
    "non_standardized_ridge_selector = SelectFromModel(non_standardized_ridge_grid.best_estimator_, max_features=10)\n",
    "non_standardized_ridge_selector.fit(X_train, y_train)\n",
    "non_standardized_ridge_important_features = X.columns[non_standardized_ridge_selector.get_support()]\n",
    "print(\"ridge_important_features for Non-standardized data\",non_standardized_ridge_important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_poly_train: (944, 10011)\n",
      "Shape of X_poly_test: (237, 10011)\n",
      "Shape of feature data (1181, 140)\n"
     ]
    }
   ],
   "source": [
    "# Apply polynomial transformation to the standardized features\n",
    "poly_order = 2\n",
    "Standardized_poly = PolynomialFeatures(degree=poly_order)\n",
    "X_poly_train = Standardized_poly.fit_transform(X_train_standardized)\n",
    "X_poly_test = Standardized_poly.fit_transform(X_test_standardized)\n",
    "#print(X_poly_test,X_poly_train)\n",
    "print(\"Shape of Standardized_X_poly_train:\", X_poly_train.shape)\n",
    "print(\"Shape of Standardized_X_poly_test:\", X_poly_test.shape)\n",
    "a = read_csv('features-bulk.csv')\n",
    "print(\"Shape of feature data\",a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardized Polynomial features\n",
    "### Lasso Polynomial Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5, estimator=Lasso(max_iter=100000),\n",
      "             param_grid={'alpha': [0.01, 0.1, 1, 10]})\n",
      "Selected features based on the best estimator for Lasso Regression:\n",
      "[[ 0.69062311 -0.16435287  2.48523132 ...  0.03415912 -0.01154667\n",
      "   0.15102762]\n",
      " [ 0.54431316 -0.61606118  2.64093718 ...  0.03845224 -0.02114642\n",
      "   0.1913755 ]\n",
      " [-1.0523973   0.89507876 -1.1207621  ... -0.0587838   0.00457194\n",
      "   0.44725836]\n",
      " ...\n",
      " [ 0.22835418 -0.53682844 -0.39703961 ...  0.08811707 -0.06909946\n",
      "   1.00499341]\n",
      " [ 0.30366614  0.06436021  0.59346923 ...  0.0343216  -0.01670287\n",
      "   0.15246784]\n",
      " [-0.1279605   1.04946008 -1.35980568 ... -0.04212789  0.06128207\n",
      "   0.22971144]]\n"
     ]
    }
   ],
   "source": [
    "# Fit Lasso regression on the polynomial features\n",
    "Standardized_lasso_grid_poly = GridSearchCV(Lasso(max_iter=100000), lasso_params, cv=5)\n",
    "Standardized_lasso_grid_poly.fit(X_poly_train, y_train)\n",
    "print(Standardized_lasso_grid_poly)\n",
    "# Perform feature selection on the polynomial features\n",
    "lasso_selector_poly = SelectFromModel(Standardized_lasso_grid_poly.best_estimator_, max_features=10)\n",
    "lasso_selector_poly.fit(X_poly_train, y_train)\n",
    "#print(lasso_selector_poly)\n",
    "\n",
    "selected_features_mask = lasso_selector_poly.get_support()\n",
    "Standardized_lasso_selected_features = X_poly_train[:, selected_features_mask]\n",
    "\n",
    "print(\"Selected features based on the best estimator for Lasso Regression:\")\n",
    "print(Standardized_lasso_selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized_lasso_poly_feature_names ['1' 'space_group' 'MagpieData minimum Number' ... 'vpa^2'\n",
      " 'vpa packing fraction' 'packing fraction^2']\n",
      "Standardized_lasso_poly_selected_features_indices_str ['23', '110', '138', '139', '3106', '5638', '9064', '9709', '9710', '10008']\n",
      "Standardized_lasso_poly_selected_features_indices_original [23, 110, 138, 139, 3106, 5638, 9064, 9709, 9710, 10008]\n",
      "Standardized_lasso_poly_important_features ['MagpieData mean MeltingT' 'MagpieData minimum GSvolume_pa' 'density'\n",
      " 'vpa' 'MagpieData mean MeltingT vpa'\n",
      " 'MagpieData mean Electronegativity vpa'\n",
      " 'MagpieData mode NdUnfilled packing fraction'\n",
      " 'MagpieData minimum GSbandgap vpa'\n",
      " 'MagpieData minimum GSbandgap packing fraction' 'vpa^2']\n"
     ]
    }
   ],
   "source": [
    "Standardized_lasso_poly_selected_features_indices = lasso_selector_poly.get_support(indices=True)\n",
    "Standardized_lasso_poly_feature_names = Standardized_poly.get_feature_names_out(X.columns)\n",
    "print(\"Standardized_lasso_poly_feature_names\",Standardized_lasso_poly_feature_names)\n",
    "Standardized_lasso_poly_selected_features_indices_str = [str(i) for i in Standardized_lasso_poly_selected_features_indices]\n",
    "print(\"Standardized_lasso_poly_selected_features_indices_str\", Standardized_lasso_poly_selected_features_indices_str)\n",
    "# Map the selected feature indices back to the original feature names\n",
    "Standardized_lasso_poly_selected_features_indices_original = [int(x) for x in Standardized_lasso_poly_selected_features_indices_str if int(x) < len(Standardized_lasso_poly_feature_names)]\n",
    "print(\"Standardized_lasso_poly_selected_features_indices_original\", Standardized_lasso_poly_selected_features_indices_original)\n",
    "# Extract the corresponding feature names\n",
    "Standardized_lasso_poly_important_features = np.array(Standardized_lasso_poly_feature_names[Standardized_lasso_poly_selected_features_indices_original]).flatten()\n",
    "# Print the important features\n",
    "print(\"Standardized_lasso_poly_important_features\", Standardized_lasso_poly_important_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Polynomial Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5, estimator=Ridge(max_iter=100000),\n",
      "             param_grid={'alpha': [0.01, 0.1, 1, 10]})\n",
      "Selected features based on the best estimator for Ridge Regression:\n",
      "[[ 0.10402081  0.69062311  0.73293489 ...  0.03415912 -0.01154667\n",
      "   0.14723193]\n",
      " [-0.06304014  0.54431316  0.73293489 ...  0.03845224 -0.02114642\n",
      "   0.16573605]\n",
      " [-1.38253848 -1.0523973  -0.58237206 ... -0.0587838   0.00457194\n",
      "  -0.25336876]\n",
      " ...\n",
      " [-0.33486813  0.22835418  0.31613877 ...  0.08811707 -0.06909946\n",
      "  -0.23428519]\n",
      " [-0.31475238  0.30366614  0.48202221 ...  0.0343216  -0.01670287\n",
      "   0.06095542]\n",
      " [-0.48116444 -0.1279605   0.31613877 ... -0.04212789  0.06128207\n",
      "   0.20987216]]\n",
      "Standardized_ridge_poly_feature_names ['1' 'space_group' 'MagpieData minimum Number' ... 'vpa^2'\n",
      " 'vpa packing fraction' 'packing fraction^2']\n",
      "Standardized_ridge_poly_selected_features_indices_str ['21', '23', '25', '138', '139', '140', '9708', '9709', '9710', '9778']\n",
      "Standardized_ridge_poly_selected_features_indices_original [21, 23, 25, 138, 139, 140, 9708, 9709, 9710, 9778]\n",
      "Standardized_ridge_poly_important_features ['MagpieData maximum MeltingT' 'MagpieData mean MeltingT'\n",
      " 'MagpieData mode MeltingT' 'density' 'vpa' 'packing fraction'\n",
      " 'MagpieData minimum GSbandgap density' 'MagpieData minimum GSbandgap vpa'\n",
      " 'MagpieData minimum GSbandgap packing fraction'\n",
      " 'MagpieData mean GSbandgap vpa']\n"
     ]
    }
   ],
   "source": [
    "# Fit Ridge regression on the polynomial features\n",
    "Standardized_poly_ridge_grid = GridSearchCV(Ridge(max_iter=100000), ridge_params, cv=5)\n",
    "Standardized_poly_ridge_grid.fit(X_poly_train, y_train)\n",
    "print(Standardized_poly_ridge_grid)\n",
    "# Perform feature selection on the polynomial features\n",
    "Standardized_poly_ridge_selector = SelectFromModel(Standardized_poly_ridge_grid.best_estimator_, max_features=10)\n",
    "Standardized_poly_ridge_selector.fit(X_poly_train, y_train)\n",
    "#print(ridge_selector_poly)\n",
    "\n",
    "Standardized_poly_ridge_selected_features_mask = Standardized_poly_ridge_selector.get_support()\n",
    "Standardized_poly_ridge_selected_features = X_poly_train[:, Standardized_poly_ridge_selected_features_mask]\n",
    "\n",
    "print(\"Selected features based on the best estimator for Ridge Regression:\")\n",
    "print(Standardized_poly_ridge_selected_features)\n",
    "\n",
    "Standardized_poly_ridge_selected_features_indices = Standardized_poly_ridge_selector.get_support(indices=True)\n",
    "Standardized_ridge_poly_feature_names = Standardized_poly.get_feature_names_out(X.columns)\n",
    "print(\"Standardized_ridge_poly_feature_names\", Standardized_ridge_poly_feature_names)\n",
    "Standardized_ridge_poly_selected_features_indices_str = [str(i) for i in Standardized_poly_ridge_selected_features_indices]\n",
    "print(\"Standardized_ridge_poly_selected_features_indices_str\", Standardized_ridge_poly_selected_features_indices_str)\n",
    "# Map the selected feature indices back to the original feature names\n",
    "Standardized_ridge_poly_selected_features_indices_original = [int(x) for x in Standardized_ridge_poly_selected_features_indices_str if int(x) < len(Standardized_ridge_poly_feature_names)]\n",
    "print(\"Standardized_ridge_poly_selected_features_indices_original\", Standardized_ridge_poly_selected_features_indices_original)\n",
    "# Extract the corresponding feature names\n",
    "Standardized_ridge_poly_important_features = np.array(Standardized_ridge_poly_feature_names[Standardized_ridge_poly_selected_features_indices_original]).flatten()\n",
    "# Print the important features\n",
    "print(\"Standardized_ridge_poly_important_features\", Standardized_ridge_poly_important_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree regressor\n",
    "Three common methods for modification or boosting are Adaboost, Gradient Boosting, and Hist-Boost (a variant of Gradient Boosting). Each method has its characteristics, and the choice depends on the specific requirements of your task. Below, I'll provide a brief explanation of each and discuss the rationale for choosing one over the others:\n",
    "\n",
    "## Adaboost (Adaptive Boosting):\n",
    "\n",
    "### Explanation: \n",
    "Adaboost is an ensemble learning technique that focuses on improving the performance of weak learners (e.g., shallow decision trees) by assigning more weight to misclassified instances. It sequentially trains a series of weak models and gives more weight to misclassified instances in each iteration.\n",
    "### Rationale: \n",
    "Adaboost is versatile and often effective in improving the accuracy of weak learners. It adapts well to noisy data and can be less prone to overfitting.\n",
    "\n",
    "## Gradient Boosting:\n",
    "### Explanation: \n",
    "Gradient Boosting builds an ensemble of decision trees sequentially, where each subsequent tree corrects the errors made by the previous ones. It minimizes a loss function (e.g., mean squared error for regression) using gradient descent.\n",
    "### Rationale: \n",
    "Gradient Boosting is powerful and widely used. It often achieves high accuracy and is suitable for a variety of tasks. It allows fine-tuning of hyperparameters to balance model complexity and performance.\n",
    "# Hist-Boost (Histogram-Based Boosting):\n",
    "\n",
    "### Explanation: \n",
    "ist-Boost is a variant of Gradient Boosting that utilizes histogram-based techniques to speed up the training process. It constructs histograms of feature values to efficiently find the best splits during tree building.\n",
    "### Rationale: \n",
    "Hist-Boost can be faster than traditional Gradient Boosting, especially when dealing with large datasets. It is efficient for datasets with a large number of samples and features.\n",
    "\n",
    "# Choice:\n",
    "\n",
    "If computational efficiency is a priority and you are dealing with a large dataset, Hist-Boost might be a good choice due to its faster training times.\n",
    "If you prioritize interpretability and simplicity or have a smaller dataset, Adaboost or traditional Gradient Boosting could be suitable.\n",
    "Consider trying all three methods and comparing their performance on your specific dataset through cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Find optimal tree depth using GridSearchCV\n",
    "param_grid = {'max_depth': np.arange(1, 21)}\n",
    "grid_search_tree = GridSearchCV(tree_reg, param_grid, cv=5, scoring='r2')\n",
    "grid_search_tree.fit(X_train, y_train)\n",
    "optimal_tree_depth = grid_search_tree.best_params_['max_depth']\n",
    "\n",
    "# Train the Decision Tree Regressor with the optimal depth\n",
    "final_tree_reg = DecisionTreeRegressor(max_depth=optimal_tree_depth, random_state=42)\n",
    "final_tree_reg.fit(X_train, y_train)\n",
    "\n",
    "# Task 2.4.2: Train Gradient Boosted Decision Tree\n",
    "gradient_boost_reg = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Find optimal parameters using GridSearchCV\n",
    "param_grid_gb = {'n_estimators': [50, 100, 150], 'learning_rate': [0.01, 0.1, 0.2]}\n",
    "grid_search_gb = GridSearchCV(gradient_boost_reg, param_grid_gb, cv=5, scoring='r2')\n",
    "grid_search_gb.fit(X_train, y_train)\n",
    "\n",
    "optimal_n_estimators = grid_search_gb.best_params_['n_estimators']\n",
    "optimal_learning_rate = grid_search_gb.best_params_['learning_rate']\n",
    "\n",
    "# Train the Gradient Boosted Decision Tree with optimal parameters\n",
    "final_gb_reg = GradientBoostingRegressor(n_estimators=optimal_n_estimators, learning_rate=optimal_learning_rate, random_state=42)\n",
    "final_gb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Store the ten most important features for both models\n",
    "tree_importances = final_tree_reg.feature_importances_\n",
    "gb_importances = final_gb_reg.feature_importances_\n",
    "\n",
    "top_10_tree_features = np.argsort(tree_importances)[-10:][::-1]\n",
    "top_10_gb_features = np.argsort(gb_importances)[-10:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "important_features based on DecisionTreeRegressor Index(['MagpieData mean MeltingT', 'vpa', 'MagpieData minimum MeltingT',\n",
      "       'MagpieData maximum GSvolume_pa', 'MagpieData maximum MeltingT',\n",
      "       'MagpieData mode NValence', 'packing fraction',\n",
      "       'MagpieData mean Electronegativity', 'MagpieData mode GSvolume_pa',\n",
      "       'MagpieData minimum MendeleevNumber'],\n",
      "      dtype='object') important_features based on GradientBoostingRegressor Index(['MagpieData mean MeltingT', 'vpa', 'density',\n",
      "       'MagpieData minimum MeltingT', 'MagpieData maximum MeltingT',\n",
      "       'MagpieData minimum Column', 'MagpieData mode Column',\n",
      "       'MagpieData mode GSvolume_pa', 'MagpieData maximum GSvolume_pa',\n",
      "       'MagpieData mean GSvolume_pa'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"important_features based on DecisionTreeRegressor\",X.columns[top_10_tree_features], \"important_features based on GradientBoostingRegressor\",X.columns[top_10_gb_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Ridge Regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized Kernel Ridge Regression R-squared: 1.0\n",
      "optimal regularization parameter = 10\n",
      "optimal kernel is poly\n",
      "optimal gamma = 0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'final_kernel_reg = KernelRidge(alpha=optimal_alpha, kernel=optimal_kernel, gamma=optimal_gamma)\\nfinal_kernel_reg.fit(X_train_standardized, y_train)'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_ridge = KernelRidge()\n",
    "param_grid_kernel = {'alpha': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly'], 'gamma': [0.1, 1, 10]}\n",
    "# GridSearchCV to find optimal parameters\n",
    "kernel_standardized = GridSearchCV(kernel_ridge, param_grid_kernel, cv=5)\n",
    "kernel_standardized.fit(X_train_standardized, y_train)\n",
    "kernel_pred_X = kernel_standardized.predict(X_test_standardized)\n",
    "kernel_y_true = y_test['K_VRH']\n",
    "kernel_y_true = kernel_standardized.predict(X_train_standardized)\n",
    "kernel_r2 = calculate_r2(kernel_y_true, kernel_y_true)\n",
    "print(\"Standardized Kernel Ridge Regression R-squared:\", kernel_r2[0])\n",
    "# Get the optimal hyperparameters\n",
    "optimal_alpha = kernel_standardized.best_params_['alpha']\n",
    "print(\"optimal regularization parameter =\",optimal_alpha)\n",
    "optimal_kernel = kernel_standardized.best_params_['kernel']\n",
    "print(\"optimal kernel is\",optimal_kernel)\n",
    "optimal_gamma = kernel_standardized.best_params_['gamma']\n",
    "print(\"optimal gamma =\",optimal_gamma)\n",
    "\n",
    "\n",
    "#final_kernel_reg = KernelRidge(alpha=optimal_alpha, kernel=optimal_kernel, gamma=optimal_gamma)\n",
    "#final_kernel_reg.fit(X_train_standardized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Cross-Validated R-squared Score: 0.8934173898090536\n"
     ]
    }
   ],
   "source": [
    "best_score = grid_search_kernel.best_score_\n",
    "print(\"Best Cross-Validated R-squared Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=1.07421e-17): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=1.20696e-17): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=5.16642e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=1.00926e-17): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=4.28205e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=8.25156e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=1.2222e-17): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=5.29984e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=1.33321e-17): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=4.97e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=6.73902e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=1.21895e-17): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=5.20706e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=4.36967e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=3.15859e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=5.28669e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=1.20575e-17): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=5.21074e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=6.65391e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=4.2536e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=8.25156e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=1.2222e-17): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=5.29984e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=1.33321e-17): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=4.97e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=6.73902e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=1.21895e-17): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=5.20706e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=4.36967e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=3.15859e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=1.14631e-17): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=1.20554e-17): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=5.1515e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=1.12625e-17): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=4.15104e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=8.25156e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=1.22237e-17): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=5.28679e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=1.328e-17): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=4.73563e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=6.73902e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=1.21895e-17): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=5.20706e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=4.36967e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n",
      "c:\\Users\\suman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:253: LinAlgWarning: Ill-conditioned matrix (rcond=3.15859e-18): result may not be accurate.\n",
      "  dual_coef = linalg.solve(K, y, assume_a=\"pos\", overwrite_a=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Standardized Kernel Ridge Regression R-squared: 1.0\n",
      "Non-Standardized optimal regularization parameter = 1\n",
      "Non-Standardized optimal kernel is linear\n",
      "Non-Standardized optimal gamma = 0.1\n"
     ]
    }
   ],
   "source": [
    "kernel_non_standardized = GridSearchCV(kernel_ridge, param_grid_kernel, cv=5, scoring='r2')\n",
    "kernel_non_standardized.fit(X_train, y_train)\n",
    "kernel_pred_X = kernel_non_standardized.predict(X_test)\n",
    "kernel_y_true = y_test['K_VRH']\n",
    "kernel_y_true = kernel_non_standardized.predict(X_train)\n",
    "kernel_r2 = calculate_r2(kernel_y_true, kernel_y_true)\n",
    "print(\"Non-Standardized Kernel Ridge Regression R-squared:\", kernel_r2[0])\n",
    "# Get the optimal hyperparameters\n",
    "optimal_alpha = kernel_non_standardized.best_params_['alpha']\n",
    "print(\"Non-Standardized optimal regularization parameter =\",optimal_alpha)\n",
    "optimal_kernel = kernel_non_standardized.best_params_['kernel']\n",
    "print(\"Non-Standardized optimal kernel is\",optimal_kernel)\n",
    "optimal_gamma = kernel_non_standardized.best_params_['gamma']\n",
    "print(\"Non-Standardized optimal gamma =\",optimal_gamma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The warning messages you're seeing are related to the ill-conditioned matrix, and they indicate that the matrix being inverted in the Ridge regression is close to being singular, which can lead to numerical instability. This can happen when the features are highly correlated or when there are redundant features.\n",
    "\n",
    "To address this issue, you can try the following approaches:\n",
    "\n",
    "Feature Scaling:\n",
    "\n",
    "Ensure that your features are scaled appropriately. Standardize the features (subtract mean and divide by standard deviation) before applying Ridge regression. This can sometimes help with numerical stability.\n",
    "Feature Selection:\n",
    "\n",
    "Consider performing feature selection to remove highly correlated or redundant features before applying Ridge regression. This can help improve the condition of the matrix.\n",
    "Regularization Parameter Adjustment:\n",
    "\n",
    "Experiment with different values of the regularization parameter (alpha). A smaller alpha might help with numerical stability. You can try values like 0.01, 0.1, or other small positive values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
